# azureopenai-intro
Resumo das diferentes configura√ß√µes e par√¢metros utilizados no Playground do Azure OpenAI

---

## **Tokeniza√ß√£o**
### O que √©?
A tokeniza√ß√£o √© o processo de **dividir texto** em **tokens**, que podem ser palavras, subpalavras, caracteres ou at√© unidades especiais.  
Este processo √© essencial para modelos de linguagem de intelig√™ncia artificial entenderem e processarem texto.  
Os modelos t√™m um limite m√°ximo de tokens por resposta. Se um texto ultrapassa esse limite, os tokens extras s√£o cortados.  

### Como funciona?
1. **Divis√£o do texto:**  
   O texto √© quebrado em partes menores (tokens), que podem ser palavras inteiras ou peda√ßos de palavras.  
   **Exemplo:** O texto *"Eu gosto de programa√ß√£o"* seria dividido em `["Eu", "gosto", "de", "programa√ß√£o", "!"]`  
2. **Uso de subpalavras (BPE - Byte Pair Encoding):**  
   Para lidar com palavras desconhecidas, pode ser utilizada a t√©cnica BPE, que quebra palavras em subpartes.  
   **Exemplo:** A palavra *"programa√ß√£o"* pode ser dividida em `["programa", "√ß√£o"]`  
3. **Transforma√ß√£o de cada token em n√∫meros:**  
   Modelos de intelig√™ncia artificial n√£o compreendem texto diretamente. Por isso, cada token √© convertido num n√∫mero (ID do token).  

### Exemplo de Tokeniza√ß√£o
**Frase:** *A tokeniza√ß√£o √© um aspeto crucial nos modelos de linguagem.*  
**Divis√£o do texto em tokens:** `["A", "token", "iza√ß√£o", "√©", "um", "asp", "eto", "crucial", "nos", "modelos", "de", "linguagem", "."]`  
**N√∫mero de tokens:** 13  
**IDs dos tokens:** `[32, 6602, 17403, 1212, 1713, 20545, 7555, 19008, 4001, 39564, 334, 108504, 13]`  

### Porque a tokeniza√ß√£o √© importante?  
‚úÖ **Efici√™ncia**: O modelo processa tokens em vez de letras individuais, acelerando o processo.  
‚úÖ **Redu√ß√£o de Mem√≥ria**: Menos tokens significam menos consumo de recursos.  
‚úÖ **Palavras Desconhecidas**: Ao dividir palavras complexas em partes menores, o modelo entende melhor termos novos.  

---

## **System Message**
### O que √©?
A **System Message** √© uma mensagem especial usada para definir o comportamento do modelo antes do in√≠cio da conversa. Diferente das mensagens do usu√°rio, essa configura√ß√£o √© invis√≠vel para quem interage com o modelo, mas influencia suas respostas.

### Como funciona?
1. **Define o papel da IA:** Pode instruir o modelo a agir como um especialista em uma √°rea espec√≠fica.  
   **Exemplo:** *"Voc√™ √© um assistente especializado em seguran√ßa cibern√©tica."*
2. **Ajusta o tom e o estilo das respostas:** Pode determinar se o modelo responde de forma formal, casual ou t√©cnica.  
   **Exemplo:** *"Responda de maneira objetiva e concisa."*
3. **Restringe ou amplia certos conte√∫dos:** Pode bloquear t√≥picos proibidos ou incentivar explica√ß√µes detalhadas.  
   **Exemplo:** *"Evite dar conselhos m√©dicos."*

### Porque a System Message √© importante?
‚úÖ **Personaliza√ß√£o**: Permite ajustar o comportamento da IA conforme a necessidade.  
‚úÖ **Controle**: Evita respostas inadequadas ou fora do escopo.  
‚úÖ **Consist√™ncia**: Garante que o modelo siga uma abordagem uniforme ao longo da intera√ß√£o.  

---

## **Temperatura vs Top-P**
### O que s√£o?
Os par√¢metros **Temperatura** e **Top-P** controlam a aleatoriedade e a criatividade das respostas do modelo.  

### Como funcionam?
1. **Temperatura (üî• Aleatoriedade):** Ajusta o qu√£o imprevis√≠vel a resposta pode ser.  
   - Valores **baixos (ex: 0.1 - 0.3)** ‚Üí Respostas mais diretas e previs√≠veis.  
   - Valores **altos (ex: 0.8 - 1.5)** ‚Üí Respostas mais criativas e variadas.  
   **Exemplo:** Pergunta: *"Qual a capital da Fran√ßa?"*  
   - `Temperatura 0.2`: *"Paris."*  
   - `Temperatura 1.0`: *"Paris, a cidade luz, famosa por sua Torre Eiffel!"*  

2. **Top-P (üéØ Nucleus Sampling):** Controla a diversidade da resposta restringindo a escolha de palavras.
   - `Top-P = 1.0` ‚Üí Considera todas as palavras poss√≠veis.  
   - `Top-P = 0.9` ‚Üí Usa apenas as palavras que somam 90% da probabilidade total.  
   - `Top-P = 0.5` ‚Üí Escolhe entre um grupo ainda mais restrito, tornando as respostas mais previs√≠veis.  

### Quando usar?
‚úÖ **Use Temperatura para ajustar aleatoriedade** (valores altos = criatividade, valores baixos = precis√£o).  
‚úÖ **Use Top-P para limitar escolhas de palavras** (valores baixos = respostas mais seguras).  
‚úÖ **Normalmente, se ajusta um ou outro, n√£o ambos ao mesmo tempo.**  

---

## **Frequency Penalty vs Presence Penalty**
### O que s√£o?
Os par√¢metros **Frequency Penalty** e **Presence Penalty** controlam como o modelo lida com palavras repetidas, evitando redund√¢ncia e incentivando diversidade.

### Como funcionam?
1. **Frequency Penalty (üîÅ Evita Repeti√ß√µes)**
   - Penaliza palavras que j√° apareceram **muitas vezes** no texto.  
   - Quanto maior o valor, menos repeti√ß√£o ocorre.  
   **Exemplo:**
   - `Frequency Penalty = 0.0`: *"O cachorro √© bonito. O cachorro gosta de brincar. O cachorro √© feliz."*  
   - `Frequency Penalty = 1.0`: *"O cachorro √© bonito. Ele gosta de brincar e √© muito feliz."*  

2. **Presence Penalty (üÜï Incentiva Novidade)**
   - Penaliza palavras que **j√° apareceram pelo menos uma vez**, empurrando o modelo para introduzir novos conceitos.  
   **Exemplo:**
   - `Presence Penalty = 0.0`: *"Paris √© uma cidade incr√≠vel. Paris tem monumentos hist√≥ricos. Paris √© um destino famoso."*  
   - `Presence Penalty = 1.0`: *"Paris √© uma cidade incr√≠vel. Seus monumentos hist√≥ricos, como a Torre Eiffel e o Louvre, atraem turistas do mundo todo."*  

### Quando usar?
‚úÖ **Use Frequency Penalty para evitar repeti√ß√µes desnecess√°rias.**  
‚úÖ **Use Presence Penalty para incentivar novas ideias e temas.**  
‚úÖ **Valores moderados (~0.5) ajudam a equilibrar coer√™ncia e criatividade.**  

---

